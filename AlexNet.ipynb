{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = glob.glob(\"./Images/n*/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Images/n02097658-silky_terrier/n02097658_26.jpg',\n",
       " './Images/n02097658-silky_terrier/n02097658_4869.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_filenames[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = defaultdict(list)\n",
    "testing_dataset = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename_with_breed = map(lambda filename: (filename.split(\"/\")[2], filename),\n",
    "                                image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n",
      "training dataset testing dataset  END-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dog_breed, breed_images in groupby(image_filename_with_breed, lambda x: x[0]):\n",
    "    \n",
    "    for i, breed_image in enumerate(breed_images):\n",
    "        if i % 5 == 0:\n",
    "            testing_dataset[dog_breed].append(breed_image[1])\n",
    "        else:\n",
    "            training_dataset[dog_breed].append(breed_image[1])\n",
    "    \n",
    "    breed_training_count = len(training_dataset[dog_breed])\n",
    "    breed_testing_count = len(testing_dataset[dog_breed])\n",
    "    \n",
    "    breed_training_count_float = float(breed_training_count)\n",
    "    breed_testing_count_float = float(breed_testing_count)\n",
    "    \n",
    "    assert round(breed_testing_count_float / (breed_training_count_float + breed_testing_count_float), 2) > 0.18, \"Not enough testing images.\"\n",
    "    \n",
    "    print \"training dataset testing dataset  END-------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/training-images/training-image-0.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-100.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-200.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-300.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-400.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-500.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-600.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-700.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-800.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-900.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1000.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1100.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1200.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1300.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1400.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1500.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1600.tfrecords------------------------------------------------------------------\n",
      "./output/training-images/training-image-1700.tfrecords------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def write_records_file(dataset, record_location):\n",
    "    \n",
    "    if not os.path.exists(record_location):\n",
    "        print(\" %s not exist, creating...\"  % (record_location))\n",
    "        os.makedirs(record_location)\n",
    "        \n",
    "    writer = None\n",
    "    \n",
    "    current_index = 0\n",
    "    \n",
    "    for breed, images_filenames in dataset.items():\n",
    "        for image_filename in images_filenames:\n",
    "            if current_index % 100 == 0:\n",
    "                if writer:\n",
    "                    writer.close()\n",
    "\n",
    "                record_filename = \"{record_location}-{current_index}.tfrecords\".format(\n",
    "                    record_location=record_location,\n",
    "                    current_index=current_index)\n",
    "\n",
    "                writer = tf.python_io.TFRecordWriter(record_filename)\n",
    "                print record_filename + \"------------------------------------------------------------------\"\n",
    "            current_index += 1\n",
    "\n",
    "            image_file = tf.read_file(image_filename)\n",
    "\n",
    "            # In ImageNet dogs, there are a few images which TensorFlow doesn't recognize as JPEGs. This\n",
    "            # try/catch will ignore those images.\n",
    "            try:\n",
    "                image = tf.image.decode_jpeg(image_file)\n",
    "            except:\n",
    "                print(image_filename)\n",
    "                continue\n",
    "\n",
    "            # Converting to grayscale saves processing and memory but isn't required.\n",
    "            grayscale_image = tf.image.rgb_to_grayscale(image)\n",
    "            resized_image = tf.image.resize_images(grayscale_image, [250, 151])\n",
    "\n",
    "            # tf.cast is used here because the resized images are floats but haven't been converted into\n",
    "            # image floats where an RGB value is between [0,1).\n",
    "            image_bytes = sess.run(tf.cast(resized_image, tf.uint8)).tobytes()\n",
    "\n",
    "            \n",
    "            #image = Image.open(image_filename)\n",
    "            #image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "            #image_bytes = image.tobytes()  # 将图片转成二进制\n",
    "            # Instead of using the label as a string, it'd be more efficient to turn it into either an\n",
    "            # integer index or a one-hot encoded rank one tensor.\n",
    "            # https://en.wikipedia.org/wiki/One-hot\n",
    "            image_label = breed.encode(\"utf-8\")\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'label': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_label])),\n",
    "                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))\n",
    "            }))\n",
    "\n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "#write_records_file(testing_dataset, \"./output/testing-images/testing-image\")\n",
    "write_records_file(training_dataset, \"./output/training-images/training-image\")\n",
    "print \"writing records_file END--------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "    tf.train.match_filenames_once(\"./output/training-images/*.tfrecords\"))\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized = reader.read(filename_queue)\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "    serialized,\n",
    "    features={\n",
    "        'label': tf.FixedLenFeature([], tf.string),\n",
    "        'image': tf.FixedLenFeature([], tf.string),\n",
    "    })\n",
    "\n",
    "record_image = tf.decode_raw(features['image'], tf.uint8)\n",
    "\n",
    "# Changing the image into this shape helps train and visualize the output by converting it to\n",
    "# be organized like an image.\n",
    "image = tf.reshape(record_image, [250, 151, 1])\n",
    "\n",
    "label = tf.cast(features['label'], tf.string)\n",
    "\n",
    "min_after_dequeue = 10\n",
    "batch_size = 3\n",
    "capacity = min_after_dequeue + 3 * batch_size\n",
    "image_batch, label_batch = tf.train.shuffle_batch(\n",
    "    [image, label], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "print \" load image form TFRcord  END---------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the images to a float of [0,1) to match the expected input to convolution2d\n",
    "float_image_batch = tf.image.convert_image_dtype(image_batch, tf.float32)\n",
    "\n",
    "conv2d_layer_one = tf.contrib.layers.convolution2d(\n",
    "    float_image_batch,\n",
    "    num_output_channels=32,     # The number of filters to generate\n",
    "    kernel_size=(5,5),          # It's only the filter height and width.\n",
    "    activation_fn=tf.nn.relu,\n",
    "    weight_init=tf.random_normal,\n",
    "    stride=(2, 2),\n",
    "    trainable=True)\n",
    "pool_layer_one = tf.nn.max_pool(conv2d_layer_one,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "\n",
    "# Note, the first and last dimension of the convolution output hasn't changed but the\n",
    "# middle two dimensions have.\n",
    "conv2d_layer_one.get_shape(), pool_layer_one.get_shape()\n",
    "print \"conv2d_layer_1 pool_layer_1  END---------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
